{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.utils import shuffle  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble model\n",
    "### model\n",
    "1. RF x 7\n",
    "2. GBDT x 11\n",
    "3. XGB x 17\n",
    "\n",
    "### feature\n",
    "1. all features\n",
    "2. used features\n",
    "3. no features\n",
    "\n",
    "### data\n",
    "1. 3 + 2\n",
    "2. second layer --> LR x 1 or [XGB x 3 (AVG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到模型的只，\n",
    "def get_x_rf(df_train, feats, label):\n",
    "    '''\n",
    "    使用9种不同的参数的models\n",
    "    '''\n",
    "    seed = 17\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    x_n_estimators = [20,100,500]\n",
    "    max_depth = 5\n",
    "    min_samples_split = 3\n",
    "    verbose = 10\n",
    "    x_random_states = [71, 91, 101]\n",
    "    \n",
    "    rf_models = []\n",
    "    data_res = {}\n",
    "    for i in range(len(x_n_estimators)):\n",
    "        for j in range(len(x_random_states)):\n",
    "            print i,j\n",
    "            clf = RandomForestClassifier(max_depth=max_depth, random_state=x_random_states[j],\n",
    "                                         min_samples_split=min_samples_split, \n",
    "                                         n_estimators=x_n_estimators[i],verbose=10)    \n",
    "            clf.fit(df_train[feats],df_train[label])\n",
    "            rf_model.append(clf)\n",
    "#             prob = clf.predict_proba(df_test[feats])\n",
    "#             data_rea['RF_%d_%d_PROB' % (i,j)] = prob\n",
    "    \n",
    "    return rf_models\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_valid(df_train,features,label,test_size=0.2):\n",
    "    '''\n",
    "    k-fold交叉验证,默认k=10\n",
    "    df_train:训练数据\n",
    "    '''\n",
    "    X_train, X_vali, y_train, y_vali = train_test_split(df_train[features], df_train[label], test_size=test_size, random_state=40000)\n",
    "    #added some parameters\n",
    "        \n",
    "    dtrain = xgb.DMatrix(X_train,label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_vali,label=y_vali)\n",
    "    watchlist = [(dtrain, 'train'),(dvalid, 'valid')]\n",
    "    \n",
    "    return dtrain, dvalid, watchlist\n",
    "\n",
    "# 得到模型的只，\n",
    "def get_x_gbdt(df_train, feats, label):\n",
    "    '''\n",
    "    使用7种不同的参数\n",
    "    '''\n",
    "    seed = 17\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    learning_rate = 0.05\n",
    "    n_estimators = 180\n",
    "    max_depth = 3\n",
    "    min_samples_split = 2\n",
    "    subsample = 0.85\n",
    "    verbose = 50\n",
    "    x_random_states = [71,91,101,2018,2019,1007,2020]\n",
    "    \n",
    "    gbdt_models = []\n",
    "    data_res = {}\n",
    "    for j in range(len(x_random_states)):\n",
    "        print j\n",
    "        clf = GradientBoostingClassifier(max_depth=max_depth, random_state=x_random_states[j],\n",
    "                                         min_samples_split=min_samples_split,learning_rate=learning_rate,\n",
    "                                         n_estimators=n_estimators,verbose=verbose)    \n",
    "        clf.fit(df_train[feats],df_train[label])\n",
    "        gbdt_models.append(clf)\n",
    "#         prob = clf.predict_proba(df_test[feats])\n",
    "#         data_rea['GBDT_%d_PROB' % (j)] = prob\n",
    "    \n",
    "    return gbdt_models\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x_xgb(df_train, feats, label):\n",
    "    '''\n",
    "    stacking的第一层，使用15个不同的seed得到15维特征，返回15个模型\n",
    "    \n",
    "    '''\n",
    "    seed = 17\n",
    "    np.random.seed(seed)\n",
    "    param = {'max_depth':3, # 基准是5 \n",
    "         'eta':0.05,\n",
    "         'gamma ':0.1,\n",
    "         'colsample_bytree':0.85, # old 0.8\n",
    "         'subsample':0.85,\n",
    "         'silent':1,\n",
    "         'eval_metric':'auc',\n",
    "         'objective':'binary:logistic',\n",
    "        }\n",
    "\n",
    "    nround = 150 # 参数求的\n",
    "    xgb_models = []\n",
    "    seeds = [71,73,91,101,2017,2018,2019,2020,10003,100007,100009,20003,200005,12345,123456]\n",
    "    for i in range(len(seeds)):\n",
    "        print('LOOP',i)\n",
    "        dbuild, dvalid, watchlist = split_train_valid(df_train, feats, test_size=0.002)\n",
    "        param['seed'] = seeds[i]\n",
    "        model = xgb.train(param, dbuild, nround, watchlist,verbose_eval=20)\n",
    "        xgb_models.append(model)\n",
    "        del dbuild, dvalid, watchlist\n",
    "        \n",
    "    return xgb_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_x_xgb_second(df_train, feats, label):\n",
    "    '''\n",
    "    stacking的第一层，使用15个不同的seed得到15维特征，返回15个模型\n",
    "    \n",
    "    '''\n",
    "    seed = 17\n",
    "    np.random.seed(seed)\n",
    "    param = {'max_depth':3, # 基准是5 \n",
    "         'eta':0.05,\n",
    "         'gamma ':0.1,\n",
    "         'colsample_bytree':0.85, # old 0.8\n",
    "         'subsample':0.85,\n",
    "         'silent':1,\n",
    "         'eval_metric':'auc',\n",
    "         'objective':'binary:logistic',\n",
    "        }\n",
    "    \n",
    "    nround = 100 # 参数求的\n",
    "    xgb_models = []\n",
    "    seeds = [71,73,91]\n",
    "    for i in range(len(seeds)):\n",
    "        print('LOOP',i)\n",
    "        dbuild, dvalid, watchlist = split_train_valid(df_train,feats,label=label, test_size=0.002)\n",
    "        param['seed'] = seeds[i]\n",
    "        model = xgb.train(param, dbuild, nround, watchlist,verbose_eval=20)\n",
    "        xgb_models.append(model)\n",
    "        del dbuild, dvalid, watchlist\n",
    "        \n",
    "    return xgb_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_models(df_train, feats, label='label'):\n",
    "    rf_models = get_x_rf(df_train, feats, label)\n",
    "    gbdt_models = get_x_gbdt(df_train, feats, label)\n",
    "    \n",
    "    rf_models = []\n",
    "    gbdt_models = []\n",
    "    \n",
    "    \n",
    "    xgb_models = get_x_xgb(df_train, feats, label)\n",
    "    \n",
    "#     return rf_models, gbdt_models, xgb_models  \n",
    "    return rf_models, gbdt_models, xgb_models\n",
    "\n",
    "def one_models_predict(df_data, feats, models, model_type):\n",
    "    data = {'uid':df_data['uid']}\n",
    "    for i in range(len(models)):\n",
    "        if model_type == 'rf':\n",
    "            prob = models[i].predeict(df_data[feats])\n",
    "            data['rf_%d_prob' % i] = prob\n",
    "        elif model_type == 'gbdt':\n",
    "            prob = models[i].predeict(df_data[feats])\n",
    "            data['gbdt_%d_prob' % i] = prob\n",
    "        else:\n",
    "            prob = models[i].predict(xgb.DMatrix(df_data[feats]))\n",
    "            data['xgb_%d_prob' % i] = prob\n",
    "    \n",
    "    return pd.DataFrame(data=data)\n",
    "    \n",
    "\n",
    "def models_predict(rf_models, gbdt_models, xgb_models, df_data, feats):\n",
    "    '''\n",
    "    注意参数的顺序\n",
    "    '''\n",
    "    df_data = one_models_predict(df_data, feats, rf_models, 'xgb')\n",
    "#     df_data = pd.merge(df_data, one_models_predict(df_data, feats, gbdt_models, 'gbdt'))\n",
    "#     df_data = pd.merge(df_data, one_models_predict(df_data, feats, xgb_models, 'xgb'))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "def models_predict_x(xgb_models, df_data, feats):\n",
    "    '''\n",
    "    注意参数的顺序\n",
    "    '''\n",
    "    df_data = one_models_predict(df_data, feats, xgb_models, 'xgb')\n",
    "#     df_data = pd.merge(df_data, one_models_predict(df_data, feats, gbdt_models, 'gbdt'))\n",
    "#     df_data = pd.merge(df_data, one_models_predict(df_data, feats, xgb_models, 'xgb'))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def get_finall_model(df_train_A, df_train_B, feats, label='label'):\n",
    "    \n",
    "#     rf_models, gbdt_models, xgb_models = get_all_models(df_train_A, feats, label)\n",
    "    xgb_models = get_x_xgb(df_train_A, feats, label)\n",
    "    \n",
    "    print len(xgb_models)\n",
    "#     df_train_b = models_predict(rf_models, gbdt_models, xgb_models, df_train_B, feats)\n",
    "    df_train_b = one_models_predict(df_train_B, feats, xgb_models, 'xgb')\n",
    "    df_train_b['label'] = df_train_B['label']\n",
    "    \n",
    "    print df_train_b.info()\n",
    "    \n",
    "    feats_b = list(df_train_b.columns)\n",
    "    if 'uid' in feats_b:\n",
    "        feats_b.remove('uid')\n",
    "    \n",
    "    print \"xgb_model final %d\" % len(xgb_models)\n",
    "    xgb_second_models = get_x_xgb_second(df_train_b, feats_b, label)\n",
    "    \n",
    "    return xgb_models, xgb_second_models\n",
    "\n",
    "\n",
    "def get_finall_score(xgb_second_models, df_test, feats, threold=0.3):\n",
    "    LOOP = len(xgb_second_models)\n",
    "    dtest  = xgb.DMatrix(df_test[feats])\n",
    "    proba_test = pd.DataFrame()\n",
    "    proba_test['uid'] = df_test['uid']\n",
    "    proba_test['score'] = [0 for i in range(len(df_test))]\n",
    "    for model in xgb_second_models:\n",
    "        proba_test['score'] += model.predict(dtest)\n",
    "    proba_test['score'] /= LOOP\n",
    "\n",
    "    proba_test = proba_test.sort_values('score',ascending=False)\n",
    "    proba_test['label'] = [0 for i in range(len(proba_test))]\n",
    "\n",
    "    proba_test.loc[proba_test['score']>threold, 'label'] = 1\n",
    "#     proba_test[['uid','label']].to_csv('../result/xresult_finall_1.csv',index=False,header=False)\n",
    "    \n",
    "    return proba_test\n",
    "\n",
    "def predict_test(df_train_A, df_train_B, df_test, feats, label='label'):\n",
    "    xgb_models, xgb_second_models = get_finall_model(df_train_A, df_train_B, feats, label)\n",
    "    \n",
    "#     print \"rf_models length = %d \" % len(rf_models)\n",
    "#     print \"gbdt_models length = %d \" % len(gbdt_models)\n",
    "    print \"xgb_models length = %d \" % len(xgb_models)\n",
    "    \n",
    "    df_test_b = models_predict_x(xgb_models, df_test, feats)\n",
    "#     df_test_b['lable'] = df_train_B['label']\n",
    "    \n",
    "    print df_test_b.info()\n",
    "    \n",
    "    feats_b = df_test_b.columns\n",
    "    \n",
    "    proba_test = get_finall_score(xgb_second_models, df_data, feats_b)\n",
    "    \n",
    "    return proba_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(df_train, label='label'):\n",
    "    '''\n",
    "    将数据集分成两部分A和B，A训练第一层model，B训练第二层模型, 2 : 3\n",
    "    写入文件，保证能够重现。\n",
    "    '''\n",
    "    df_train_one = df_train[df_train[label] == 1]\n",
    "    df_train_zero = df_train[df_train[label] == 0]\n",
    "    \n",
    "    df_train_one_A = df_train_one.sample(int(len(df_train_one)*0.6))\n",
    "    df_train_one_B = df_train_one[~df_train_one['uid'].isin(df_train_one_A['uid'])]\n",
    "    \n",
    "    df_train_zero_A = df_train_zero.sample(int(len(df_train_zero)*0.6))\n",
    "    df_train_zero_B = df_train_zero[~df_train_zero['uid'].isin(df_train_zero_A['uid'])]\n",
    "    \n",
    "    df_train_A = df_train_one_A.append(df_train_zero_A)\n",
    "    df_train_B = df_train_one_B.append(df_train_zero_B)\n",
    "    \n",
    "    df_train_A = shuffle(df_train_A)\n",
    "    df_train_B = shuffle(df_train_B)\n",
    "    \n",
    "    df_train_A.to_csv('../sdata/df_train_A.csv',index=False)\n",
    "    df_train_B.to_csv('../sdata/df_train_B.csv',index=False)\n",
    "    \n",
    "    return df_train_A, df_train_B\n",
    "\n",
    "def get_data(add_data=False):\n",
    "    df_train_voice_feat = pd.read_csv('../xdata/df_train_voice_feat.csv')\n",
    "    df_test_voice_feat = pd.read_csv('../xdata/df_testB_voice_feat.csv')\n",
    "    if add_data==True:\n",
    "        df_trainA = pd.read_csv('../xdata/df_testA_label.csv')\n",
    "        df_trainA_voice_feat = pd.read_csv('../xdata/df_testA_voice_feat.csv')\n",
    "        \n",
    "    \n",
    "    df_train_sms_feat = pd.read_csv('../xdata/df_train_sms_feat.csv')\n",
    "    df_test_sms_feat = pd.read_csv('../xdata/df_testB_sms_feat.csv')\n",
    "    if add_data==True:\n",
    "        df_trainA_sms_feat = pd.read_csv('../xdata/df_testA_sms_feat.csv')\n",
    "    \n",
    "    df_train_sms_feat.drop('label',axis=1,inplace=True)\n",
    "    \n",
    "    df_train_wa_feat = pd.read_csv('../xdata/df_train_wa_feat.csv')\n",
    "    df_test_wa_feat = pd.read_csv('../xdata/df_testB_wa_feat.csv')\n",
    "    if add_data==True:\n",
    "        df_trainA_wa_feat = pd.read_csv('../xdata/df_testA_wa_feat.csv')\n",
    "\n",
    "    df_train_wa_feat.drop('label',axis=1,inplace=True)\n",
    "    \n",
    "    df_train_voice_sms_wa_feat = pd.read_csv('../xdata/df_train_voice_sms_wa_feat.csv')\n",
    "    df_test_voice_sms_wa_feat = pd.read_csv('../xdata/df_testB_voice_sms_wa_feat.csv')\n",
    "    if add_data==True:\n",
    "        df_trainA_voice_sms_wa_feat = pd.read_csv('../xdata/df_testA_voice_sms_wa_feat.csv')\n",
    "    \n",
    "#     df_train_voice_sms_wa_feat.drop('label',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    df_train = pd.merge(df_train_voice_feat, df_train_sms_feat, on='uid', how='left')\n",
    "    df_test = pd.merge(df_test_voice_feat, df_test_sms_feat, on='uid', how='left')\n",
    "    \n",
    "    df_train = pd.merge(df_train, df_train_wa_feat, on='uid', how='left')\n",
    "    df_test = pd.merge(df_test, df_test_wa_feat, on='uid', how='left')\n",
    "    \n",
    "    df_train = pd.merge(df_train, df_train_voice_sms_wa_feat, on='uid', how='left')\n",
    "    df_test = pd.merge(df_test, df_test_voice_sms_wa_feat, on='uid', how='left')\n",
    "    \n",
    "    \n",
    "    if add_data==True:\n",
    "        df_trainA = pd.merge(df_trainA, df_trainA_voice_feat, on='uid', how='left')\n",
    "        df_trainA = pd.merge(df_trainA, df_trainA_sms_feat, on='uid', how='left')\n",
    "        df_trainA = pd.merge(df_trainA, df_trainA_wa_feat, on='uid', how='left')\n",
    "        df_trainA = pd.merge(df_trainA, df_trainA_voice_sms_wa_feat, on='uid', how='left')\n",
    "        \n",
    "#         df_trainA = df_trainA[df_trainA['label']==1]\n",
    "        df_trainA = df_trainA[:66]\n",
    "        df_train = df_train.append(df_trainA)\n",
    "\n",
    "    df_train.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "    df_test.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "\n",
    "    df_train.fillna(0,inplace=True)\n",
    "    df_test.fillna(0,inplace=True)\n",
    "    \n",
    "    df_train.drop('wa_all_wa_name_little_wite',axis=1,inplace=True)\n",
    "    df_train.drop('wa_all_wa_name_many_wite',axis=1,inplace=True)\n",
    "    df_train.drop('wa_all_wa_name_little_wite_risk',axis=1,inplace=True)\n",
    "    df_train.drop('wa_all_wa_name_many_wite_risk',axis=1,inplace=True)\n",
    "\n",
    "    df_test.drop('wa_all_wa_name_little_wite',axis=1,inplace=True)\n",
    "    df_test.drop('wa_all_wa_name_many_wite',axis=1,inplace=True)\n",
    "    df_test.drop('wa_all_wa_name_little_wite_risk',axis=1,inplace=True)\n",
    "    df_test.drop('wa_all_wa_name_many_wite_risk',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    df_train.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "    df_test.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "\n",
    "    df_train.fillna(0,inplace=True)\n",
    "    df_test.fillna(0,inplace=True)\n",
    "    \n",
    "    # 数据分成A和B的两部分\n",
    "    split_data(df_train)\n",
    "    \n",
    "    df_train.to_csv('../sdata/df_train_plus.csv',index=False)\n",
    "    df_test.to_csv('../sdata/df_test_plus.csv',index=False)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "    \n",
    "    \n",
    "def split_train_valid(df_train, feats, label='label', test_size=0.2):\n",
    "    '''\n",
    "    k-fold交叉验证,默认k=10\n",
    "    df_train:训练数据\n",
    "    '''\n",
    "    X_train, X_vali, y_train, y_vali = train_test_split(df_train[feats], df_train[label], test_size=test_size, random_state=40000)\n",
    "    #added some parameters\n",
    "    \n",
    "#     dtrain = df_train.iloc[train_list]\n",
    "#     dvali =  df_train.iloc[vali_list]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train,label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_vali,label=y_vali)\n",
    "    watchlist = [(dtrain, 'train'),(dvalid, 'valid')]\n",
    "    \n",
    "    return dtrain, dvalid, watchlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = get_data(add_data=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    df_train_A = pd.read_csv('../sdata/df_train_A.csv')\n",
    "    df_train_B = pd.read_csv('../sdata/df_train_B.csv')\n",
    "    df_test = pd.read_csv('../sdata/df_test.csv')\n",
    "    \n",
    "    df_train_A.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "    df_train_B.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "    df_test.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "\n",
    "    df_train_A.fillna(0,inplace=True)\n",
    "    df_train_B.fillna(0,inplace=True) \n",
    "    df_test.fillna(0,inplace=True)\n",
    "        \n",
    "    feats = list(df_test.columns)\n",
    "    feats.remove('uid')\n",
    "    \n",
    "    df_ans = predict_test(df_train_A, df_train_B, df_test, feats, label='label')\n",
    "    \n",
    "    \n",
    "def run_x():\n",
    "    df_train = pd.read_csv('../sdata/df_train_plus.csv')\n",
    "    df_test = pd.read_csv('../sdata/df_test_plus.csv')\n",
    "    \n",
    "    df_train.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "    df_test.replace([np.inf,-np.inf], 0, inplace=True)\n",
    "\n",
    "    df_train.fillna(0,inplace=True)\n",
    "    df_test.fillna(0,inplace=True)\n",
    "        \n",
    "    feats = list(df_test.columns)\n",
    "    feats.remove('uid')\n",
    "    \n",
    "    print len(feats)\n",
    "    \n",
    "    xgb_models = get_x_xgb(df_train,feats,label='label')\n",
    "    \n",
    "    proba_test = get_finall_score(xgb_models, df_test, feats, threold=0.27)\n",
    "    \n",
    "    proba_test[['uid','label']].to_csv('../result/xresult_finall_x_1.csv',index=False,header=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336\n",
      "('LOOP', 0)\n",
      "[0]\ttrain-auc:0.841473\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.916286\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.932813\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.944752\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954278\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962482\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969291\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974818\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976594\tvalid-auc:1\n",
      "('LOOP', 1)\n",
      "[0]\ttrain-auc:0.826296\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.912754\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.937038\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.946125\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.955165\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.963651\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969724\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974971\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.977279\tvalid-auc:1\n",
      "('LOOP', 2)\n",
      "[0]\ttrain-auc:0.839682\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.911993\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.932458\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.94429\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954275\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962556\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968368\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.973824\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.975684\tvalid-auc:1\n",
      "('LOOP', 3)\n",
      "[0]\ttrain-auc:0.829816\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.914377\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.93426\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.945497\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954623\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962483\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969708\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974577\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976468\tvalid-auc:1\n",
      "('LOOP', 4)\n",
      "[0]\ttrain-auc:0.830661\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.912259\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.934779\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.945203\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954517\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.963291\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969725\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.975284\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.977564\tvalid-auc:1\n",
      "('LOOP', 5)\n",
      "[0]\ttrain-auc:0.840963\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.912091\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.936415\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.946631\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.955317\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.963463\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.96993\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.975213\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.977312\tvalid-auc:1\n",
      "('LOOP', 6)\n",
      "[0]\ttrain-auc:0.827688\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.913859\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.932723\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.943356\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.953981\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962057\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969064\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974255\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976294\tvalid-auc:1\n",
      "('LOOP', 7)\n",
      "[0]\ttrain-auc:0.838316\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.914272\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.934526\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.944386\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.953079\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962346\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.96823\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.973545\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.975766\tvalid-auc:1\n",
      "('LOOP', 8)\n",
      "[0]\ttrain-auc:0.826748\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.913466\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.933605\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.945859\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.95441\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.96182\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968984\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974172\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976102\tvalid-auc:1\n",
      "('LOOP', 9)\n",
      "[0]\ttrain-auc:0.826955\tvalid-auc:1\n",
      "[20]\ttrain-auc:0.914737\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.936293\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.944591\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954382\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.961899\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968668\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974501\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976634\tvalid-auc:1\n",
      "('LOOP', 10)\n",
      "[0]\ttrain-auc:0.828949\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.912979\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.93314\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.944205\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.953011\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.961244\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968643\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974293\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976236\tvalid-auc:1\n",
      "('LOOP', 11)\n",
      "[0]\ttrain-auc:0.838752\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.913886\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.933256\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.94426\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.954406\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.963462\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.969285\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974316\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976411\tvalid-auc:1\n",
      "('LOOP', 12)\n",
      "[0]\ttrain-auc:0.835477\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.915636\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.934179\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.945083\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.95403\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962418\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968645\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.97375\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.97657\tvalid-auc:1\n",
      "('LOOP', 13)\n",
      "[0]\ttrain-auc:0.825336\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.913701\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.932298\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.943377\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.953938\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.96215\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968631\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974826\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976892\tvalid-auc:1\n",
      "('LOOP', 14)\n",
      "[0]\ttrain-auc:0.839476\tvalid-auc:0.944444\n",
      "[20]\ttrain-auc:0.914564\tvalid-auc:1\n",
      "[40]\ttrain-auc:0.933522\tvalid-auc:1\n",
      "[60]\ttrain-auc:0.944598\tvalid-auc:1\n",
      "[80]\ttrain-auc:0.953699\tvalid-auc:1\n",
      "[100]\ttrain-auc:0.962483\tvalid-auc:1\n",
      "[120]\ttrain-auc:0.968997\tvalid-auc:1\n",
      "[140]\ttrain-auc:0.974401\tvalid-auc:1\n",
      "[149]\ttrain-auc:0.976621\tvalid-auc:1\n"
     ]
    }
   ],
   "source": [
    "run_x()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2302\n",
       "1     697\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_test = pd.read_csv('../result/xresult_finall_x_1.csv')\n",
    "\n",
    "proba_test['1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2aad49310cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# np.finfo(np.float32).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
